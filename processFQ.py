"""

Generated by Colin Florian on 9/20/24

This script performs helper functions for processing fastq files.
It contains classes of callable functions.

"""

import sys
import os
import pandas as pd
import numpy as np
import dask.dataframe as dd
import subprocess
from itertools import repeat, chain

class processFQ():

	#def __init__(self, directory, file_list=None):
	def __call__(self, directory=None, file_list=None):
		self.directory = ''
		#self.directory = directory # self.directory is the directory where data is stored
		self.file_list = [] # self.file_list a list of fastq files in the data directory
	
	@staticmethod
	def generateFileList(directory):
	#def generateFileList(self, directory):

		directory = os.fsencode(directory)
		all_files = [os.fsdecode(file) for file in os.listdir(directory)]
		file_list = []
		for file in all_files:
			if file.endswith('.fastq') or file.endswith('.fastq.gz'):
				file_list.append(file)

		#self.file_list = file_list
		return file_list


	#function for unzipping .gz files
	@staticmethod
	def unzip_gz(file):

		process = subprocess.run(['file', file], stdout=subprocess.PIPE, universal_newlines=True) # returns information about the provied fastq or fastq.gz file
		gzipped = False # presume file as not being zipped
		# if the file appears to be gzip'd then unzip and save the name of the unzipped file to remove later.
		#if 'gzip compressed data' in process.stdout:
		if 'gzip compatible' in process.stdout:
			print('+ Unzipping gzip fastq...')
			gzipped = True # the provided file was gzipped, and we will remove the temporary file after reading out information
			subprocess.run(['gunzip', '-k', file], stdout=subprocess.PIPE, universal_newlines=True) # unzip the file, keeping the original
			process = subprocess.run(['gunzip', '-l', file], stdout=subprocess.PIPE, universal_newlines=True) # get the name of the new unzipped file 
			unzipped_file = process.stdout#.split(' ')[-1].rstrip() # reassign the path pointing to the read file
		#else:
		#	read_file_path_alt = read_file_path

	@staticmethod
	def trim(file_list, args):
		'''
		Generate and run adapter trimming script.
		'''
		# should separate R1/R2.fastq.gz as long as there aren't any other "." in the file names
		files = [file.split('.') for file in file_list]
		
		# make lists of read 1/2 to make columns of a dataframe -> tab delimited text file
		r1 = []
		r2 = []
		for fileSplit in files:
			if fileSplit[0].endswith('R1') or fileSplit[0].endswith('R1_001'):
				r1.append('.'.join(fileSplit))
			elif fileSplit[0].endswith('R2') or fileSplit[0].endswith('R2_001'):
				r2.append('.'.join(fileSplit))
		
		# sort lists so that fastqs are in the correct ordered pairs
		r1 = sorted(r1)
		r2 = sorted(r2)

		outDF = pd.DataFrame({'R1': r1, 'R2': r2})
		outDF.to_csv(args.directory + '/lookup_trim.txt', sep = '\t', index = False, header = None)

		# generate adapter file list
		adaptList = []
		if args.trim3primeR1 != 'None':
			adaptList.append('-a file:%s' %(args.trim3primeR1))
		if args.trim5primeR1 != 'None':
			adaptList.append('-g file:%s' %(args.trim5primeR1))
		if args.trim3primeR2 != 'None':
			adaptList.append('-A file:%s' %(args.trim3primeR2))
		if args.trim5primeR1 != 'None':
			adaptList.append('-G file:%s' %(args.trim5primeR2))

		with open('trim.sh', 'w') as f:
			f.write('#!/bin/bash\n#SBATCH --cpus-per-task=%s\n' %(args.processors if args.processors else str(1)) + 
				'#SBATCH --mem-per-cpu=%sG\n' %(args.memory if args.memory else str(8)) +
				'#SBATCH --array=1-%s' %(str(outDF.shape[0])) +
				'#SBATCH -o trim.out\n#SBATCH -e trim.err\n\n' + 
				'cd %s \n\n' %(args.directory) + 'read read1	read2 < <( sed -n ${SLURM_ARRAY_TASK_ID}p $1 )\n\n' +
				'mkdir trimmed\n' +
				'trim1=${read1/R1_001.fastq.gz/trimmed_R1.fastq.gz}\n' +
				'trim2=${read2/R2_001.fastq.gz/trimmed_R2.fastq.gz}\n' +
				'cutadapt %s' %(' '.join(str(x) for x in adaptList)) + 
				' -j %s ' %(args.processors if args.processors else str(1)) + 
				' -o trimmed/${trim1}' + 
				' -p trimmed/${trim2}' + 
				' ${read1} ${read2}') 

		subprocess.run('sbatch trim.sh lookup_trim.txt', shell=True)

	@staticmethod
	def lookupPandaseq(file_list, directory, args):

		# should separate R1/R2.fastq.gz as long as there aren't any other "." in the file names
		files = [file.split('.') for file in file_list]
		
		# make lists of read 1/2 to make columns of a dataframe -> tab delimited text file
		r1 = []
		r2 = []
		for fileSplit in files:
			if fileSplit[0].endswith('R1') or fileSplit[0].endswith('R1_001'):
				r1.append('.'.join(fileSplit))
			elif fileSplit[0].endswith('R2') or fileSplit[0].endswith('R2_001'):
				r2.append('.'.join(fileSplit))
		
		# sort lists so that fastqs are in the correct ordered pairs
		r1 = sorted(r1)
		r2 = sorted(r2)

		outDF = pd.DataFrame({'R1': r1, 'R2': r2})
		outDF.to_csv(args.directory + '/lookup_pandaseq.txt', sep = '\t', index = False, header = None)

		# generate a job array for merging the fastq files
		with open('submit_pandaseq.sh', 'w') as f:
			f.write('#!/bin/bash' + 
				'\n#SBATCH --cpus-per-task=1' + 
				'\n#SBATCH --mem-per-cpu=1G' + 
				'\n#SBATCH --array=1-%s' %(str(outDF.shape[0])) +
				'\n#SBATCH -o submit_pandaseq.out' +
				'\n#SBATCH -e submit_pandaseq.err\n' + 
				'cd %s\n\n' %(args.directory)  + 'mkdir pandaseq\n\n' +
				'read read1	read2 < <( sed -n ${SLURM_ARRAY_TASK_ID}p lookup_pandaseq.txt )\n\n' +
				# 'mv %s/pandaseq.sh %s' %(directory, args.directory) +
				'sbatch pandaseq.sh %s ' %(args.directory) + ' ${read1} ${read2}') 
		
		# generate pandaseq shell script using sys.args provided by user
		with open('pandaseq.sh', 'w') as f:
			f.write('#!/bin/bash\n#SBATCH --cpus-per-task=%s\n' %(args.processors if args.processors else str(1)) + 
				'#SBATCH --mem-per-cpu=%sG' %(args.memory if args.memory else str(8)) +
				'\n#SBATCH -o pandaseq.out\n#SBATCH -e pandaseq.err\n' + 
				'cd %s\n\n' %(args.directory)  + #'read read1	read2 < <( sed -n ${SLURM_ARRAY_TASK_ID}p lookup_pandaseq.txt )\n\n' +
				'pandaseq -T %s' %(args.processors if args.processors else str(1)) + '-F -f $2 -r $3 -w pandaseq/${2/trimmed_R1.fastq.gz/merged.fastq}') 

		# move pandaseq scripts after generation
		subprocess.run('mv %s/submit_pandaseq.sh %s' %(directory, args.directory) , shell=True)
		subprocess.run('mv %s/pandaseq.sh %s' %(directory, args.directory) , shell=True)

	@staticmethod
	def runPandaseq(wkdir, lookup):
		# 1st arg is directory
		# 2nd and 3rd are R1 and R2, respectively
		subprocess.run('sbatch %ssubmit_pandaseq.sh %s' %(wkdir, wkdir) + ' lookup_pandaseq.txt', shell=True)


	@staticmethod
	def countLookup(file_list, args, baseDir):

		directory = args.directory + '/pandaseq/'

		names = pd.read_csv(baseDir + '/' + args.names, sep = '\t', header = None)

		# I know this is clunky, but it works
		# build the job array lookup.txt file for the specified number of partitions
		fq_names = []
		part_num = []
		sample_names = []
		for idx, names in zip(names[0], names[1]):
			for num in range(args.partitions):
				for fastq in file_list:
					if str(idx) in fastq:
						fq_names.append(fastq)
						part_num.append(num)
						sample_names.append(names + '_' + str(num+1))

		sample_df = pd.DataFrame({'fastq': fq_names, 'part': part_num, 'name': sample_names})
		
		sample_df.to_csv(args.directory + '/pandaseq/lookup.txt', sep = '\t', index = False, header = None)

		# return length of the data frame to know the size of the job array
		return sample_df.shape[0]

	@staticmethod
	def countBatch(numJobs, args, directory):

		with open(args.directory + '/pandaseq/count_array.sh', 'w') as f:
			f.write('#!/bin/bash\n#SBATCH --mem=%sG\n' %(args.memory if args.memory else str(8)) + 
				'#SBATCH --cpus-per-task=%s\n' %(args.processors if args.processors else str(1)) + 
				'#SBATCH --array=1-%s\n' %(numJobs) + 
				'#SBATCH -e count_array.err\n#SBATCH -o count_array.out\n\n' + 
				'cd %s\n\n' %(args.directory + '/pandaseq') + 
				'read read_file	partition	sample < <( sed -n ${SLURM_ARRAY_TASK_ID}p lookup.txt )\n\n' +
				'mkdir counts\n' + 
				'mkdir merged_counts\n\n' +
				'mv %s .\n\n' %(directory + '/' + args.reference) + 
				'mv %s/string_match* .\n\n' %(directory) +
				'mv %s/MPRA_count.py .\n\n' %(directory) +
				'python %s/MPRA_count.py ${read_file} %s ${sample} ${partition} %s' %(args.directory + '/pandaseq', args.reference, args.partitions))
		print('count_array.sh written')

	@staticmethod
	def runCount(directory):
		subprocess.run('sbatch %s/count_array.sh' %(directory + '/pandaseq'), shell=True)

	@staticmethod
	def makeMerge(args, directory):

		with open(args.directory + '/pandaseq/counts/merge.sh', 'w') as f:
			f.write('#!/bin/bash\n#SBATCH --mem=8G\n' + 
				'#SBATCH --cpus-per-task=1\n' + 
				'#SBATCH -e merge.err\n#SBATCH -o merge.out\n\n' + 
				'cd %s\n\n' %(args.directory + '/pandaseq/counts') + 
				# 'mv %s/merge.py .\n\n' %(args.directory) + 
				# 'python merge.py %s %s' %(args.directory + '/pandaseq/counts', args.directory + '/pandaseq/merged_counts')) 
				'mv %s/merge.py .\n\n' %(directory) + 
				'python merge.py %s %s' %(args.directory + '/pandaseq/counts', args.directory + '/pandaseq/merged_counts')) 